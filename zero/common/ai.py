#For the Paramseters of text generations
from typing import *
import re

import torch
import random
import numpy

from transformers import GPT2LMHeadModel, GPT2Tokenizer


class AI(object):
    
    #The class responsible for handling raw text-generation using a gpt-2 model.
    
    def __init__(
            self,
            model_path=None,
            use_gpu=True,
    ):
        assert model_path, "No model specified!"

        self.model_path = str(model_path)
        self.use_gpu = torch.cuda.is_available() and use_gpu
        self.dtype = torch.float16
        self.device = torch.device("cuda" if self.use_gpu else "cpu")

        seed = random.randint(0, 2147483647)
        numpy.random.seed(seed)
        torch.random.manual_seed(seed)

        self.tokenizer = GPT2Tokenizer.from_pretrained(self.model_path)
        self.eos_token_id = self.tokenizer.encode('<|endoftext|>')[0]

        self.model = GPT2LMHeadModel.from_pretrained(self.model_path)
        self.model.to(self.dtype).to(self.device)
        self.model.eval()

    @property
    def model_info(self) -> str:
        return f'{self.dtype} precision model running on {"gpu" if self.use_gpu else "cpu"}' 

    def generate(
            self,
            text: str,
            max_length: int,
            beam_searches: int,
            temperature: float,
            top_k: float,
            top_p: float,
            repetition_penalty: float,
    ) -> str:
        """
        Generates a raw, unaltered string from a single input.

        :Parameters text: The text to use to generate the string.
        :Parameters max_length: The maximum length of string to generate.
        :Parameters beam_searches: The number of beam searches to perform.
        :Parameters temperature: The temperature used by the sampling algorithm.
        :Parameters top_k: The top_k value used by the sampling algorithm.
        :Parameters top_p: The top_p value used by the sampling algorithm.
        :Parameters repetition_penalty: The repetition penalty. 1.0 is no penalty.
        :return: An unaltered string generated by the AI.
        """
        input_ids = self.tokenizer.encode(text, return_tensors='pt')
        input_ids = input_ids.to(self.device)
        input_len = len(input_ids[0])
        result = self.model.generate(
            input_ids=input_ids,
            min_length=input_len,
            max_length=input_len+max_length,
            do_sample=True,
            num_beams=beam_searches,
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            repetition_penalty=repetition_penalty,
            eos_token_id=self.eos_token_id,
        )
        return self.tokenizer.decode(
            result[0][input_len:],
            clean_up_tokenization_spaces=False,
            skip_special_tokens=True,
        )
